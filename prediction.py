# -*- coding: utf-8 -*-
"""Employee_Attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12qGr9FZ1_EAkcqJ_b2TjYe4hnEp5GLWG

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech

- **Nama:** Johanadi Santoso
- **Email:** johanadisantoso1@gmail.com
- **ID Dicoding:** johanadisantoso

## Persiapan

### Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sqlalchemy import create_engine

"""### Menyiapkan data yang akan digunakan
Dataset dapat diakses melalui tautan ini: https://github.com/dicodingacademy/dicoding_dataset/blob/main/employee/employee_data.csv

## Data Understanding
"""

# Load dataset
# df = pd.read_csv('https://github.com/dicodingacademy/dicoding_dataset/raw/main/employee/employee_data.csv')
df = pd.read_csv('employee_data.csv')

# Menampilkan ukuran (baris, kolom) dari dataset
print("(baris, kolom):", df.shape)

# Menampilkan ringkasan informasi data
df.info()

# Menampilkan statistik deskriptif untuk semua kolom, termasuk numerik dan kategorikal
df.describe(include='all')

"""## Data Preparation / Preprocessing"""

# Menampilkan 5 baris pertama dari dataset
df.head()

"""### Check Missing Values"""

# Menghitung jumlah dan persentase data yang hilang di setiap kolom
missing_values = df.isna().sum()
missing_percentage = (missing_values / len(df)) * 100

# Membuat DataFrame untuk merangkum informasi missing value
missing_df = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage (%)': missing_percentage.round(2)
})

missing_df

"""
### **Insight:**

####  Temuan:

* Kolom **`Attrition`**, yang merupakan **target variabel** dalam proses pelatihan model, memiliki **412 missing values**, setara dengan **28.03%** dari total data.
* Seluruh kolom lainnya tidak memiliki missing values.

####  Dampak:

* Kehilangan data pada kolom target **berisiko menurunkan kualitas pelatihan model** dan dapat menyebabkan **bias prediksi** jika tidak ditangani dengan benar.
* Imputasi tidak direkomendasikan karena `Attrition` adalah label klasifikasi, dan pengisian nilai secara estimasi dapat menghasilkan **label palsu**.

####  Tindakan:

* **Menghapus seluruh baris** dengan nilai `Attrition` yang kosong.
* Tujuannya adalah untuk:

  * Menjaga integritas label dalam dataset pelatihan.
  * Meningkatkan kualitas dan akurasi model prediktif.

"""

# Menghapus baris dari DataFrame yang memiliki nilai NaN (kosong) pada kolom 'Attrition'
df = df.dropna(subset=['Attrition'])

"""### Check Duplicate"""

# Menghitung jumlah baris duplikat dalam DataFrame
jumlah_duplikat = df[df.duplicated()].shape[0]

# Menghitung persentase baris duplikat terhadap total baris
persentase_duplikat = (jumlah_duplikat / df.shape[0]) * 100

# Menampilkan hasil
print(f"Jumlah data duplikat: {jumlah_duplikat}")
print(f"Persentase data duplikat: {persentase_duplikat:.2f}%")

"""### Cek Nilai Unik di Setiap Kolom Kategorikal"""

# Daftar kolom kategorikal
categorical_columns = [
    'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

# List untuk menampung data
summary_data = []

# Mengisi list dengan nama kolom, jumlah nilai unik, dan daftar nilainya
for column in categorical_columns:
    unique_vals = df[column].unique()
    num_unique = df[column].nunique()
    summary_data.append({
        'Kolom': column,
        'Jumlah Unik': num_unique,
        'Nilai Unik': list(unique_vals)
    })

# Buat DataFrame ringkasan
summary_df = pd.DataFrame(summary_data)

# Tampilkan tabel ringkasan
summary_df

"""###  Mengubah kolom kategorikal menjadi tipe kategori"""

# Mengubah setiap kolom kategorikal menjadi tipe kategori
for column in categorical_columns:
    df[column] = df[column].astype('category')

# Menampilkan ringkasan informasi data
df.info()

"""### Feature Engineering

Dalam tahap ini, data dipisahkan berdasarkan tipe fiturnya, yaitu numerik, kategorikal nominal, dan kategorikal ordinal. Tujuan pembagian ini adalah untuk mempermudah proses encoding dan transformasi sesuai karakteristik masing-masing fitur.
"""

# Menunjukkan stabilitas dalam peran saat ini (StabilityInRole)
df['StabilityInRole'] = df['YearsInCurrentRole'] / df['YearsAtCompany']

# Menilai tingkat kesetiaan terhadap atasan langsung (LoyaltyToManager)
df['LoyaltyToManager'] = df['YearsWithCurrManager'] / df['YearsAtCompany']

# Menghitung rata-rata pelatihan yang diikuti setiap tahun (AvgTrainingPerYear)
df['AvgTrainingPerYear'] = df['TrainingTimesLastYear'] / df['YearsAtCompany']

# Memperkirakan usia saat mulai bekerja (AgeWhenStarted)
df['AgeWhenStarted'] = df['Age'] - df['TotalWorkingYears']

# Mengetahui rata-rata durasi bekerja di tiap perusahaan (AvgYearsPerCompany)
df['AvgYearsPerCompany'] = df['TotalWorkingYears'] / df['NumCompaniesWorked']

# Rasio pendapatan harian terhadap jarak rumah ke kantor (IncomePerKm)
df['IncomePerKm'] = df['DailyRate'] / df['DistanceFromHome']

# Tingkat loyalitas karyawan terhadap perusahaan (CompanyLoyalty)
df['CompanyLoyalty'] = df['YearsAtCompany'] / df['TotalWorkingYears']

# Mengukur seberapa sering karyawan dipromosikan (PromotionFrequency)
df['PromotionFrequency'] = df['YearsAtCompany'] / (df['YearsSinceLastPromotion'] + 1)

# Rata-rata penghasilan bulanan berdasarkan masa kerja (AvgMonthlyIncomePerYear)
df['AvgMonthlyIncomePerYear'] = df['MonthlyIncome'] / df['YearsAtCompany']

"""### Menghapus Kolom yang Tidak Digunakan"""

df = df.drop(columns=['EmployeeId', 'Over18', 'EmployeeCount', 'StandardHours'])

df.describe(include='all')

# Fungsi untuk memeriksa apakah terdapat nilai tak terhingga (infinity) dalam suatu kolom
def cek_nilai_inf(series):
    try:
        # Mengembalikan True jika ada nilai tak terhingga, False jika tidak
        return np.isinf(series).any()
    except TypeError:
        # Jika terjadi TypeError, artinya input bukan tipe yang dapat diperiksa, kembalikan False
        return False

# Mengidentifikasi kolom-kolom dalam DataFrame yang mengandung nilai tak terhingga (infinity)
kolom_dengan_inf = [kolom for kolom in df.columns if cek_nilai_inf(df[kolom])]

# Memeriksa apakah ada kolom yang mengandung nilai tak terhingga
if kolom_dengan_inf:
    print("Kolom-kolom yang mengandung nilai tak terhingga adalah:")
    print(kolom_dengan_inf)
else:
    print("Tidak ditemukan kolom yang mengandung nilai tak terhingga.")

df[kolom_dengan_inf] = df[kolom_dengan_inf].replace([np.inf, -np.inf], np.nan)

# Step 2: Ganti NaN dengan nilai maksimum kolom yang valid
for kolom in kolom_dengan_inf:
    max_val = df[kolom].max(skipna=True)
    df[kolom] = df[kolom].fillna(max_val)

# Mengidentifikasi kolom-kolom dalam DataFrame yang memiliki nilai tak terhingga (infinity)
kolom_dengan_inf = [kolom for kolom in df.columns if cek_nilai_inf(df[kolom])]

# Memeriksa apakah ada kolom yang terdeteksi mengandung nilai tak terhingga
if kolom_dengan_inf:
    print("Kolom-kolom yang memiliki nilai tak terhingga adalah:")
    print(kolom_dengan_inf)
else:
    print("Semua kolom tidak mengandung nilai tak terhingga.")

df.describe(include='all')

"""### Exploratory Data Analysis"""

# Buat figure
plt.figure(figsize=(20, 20))

# Hitung korelasi
correlation_matrix = df.corr(numeric_only=True)

# Plot heatmap
sns.heatmap(correlation_matrix,
            annot=True,  # Tampilkan nilai
            fmt='.2f',   # Format 2 desimal
            cmap='coolwarm', # Color scheme
            square=True,  # Bentuk kotak
            linewidths=0.5, # Garis pemisah
            annot_kws={'size': 8}, # Ukuran teks
            cbar_kws={'shrink': .8}) # Ukuran colorbar

# Atur tampilan
plt.title('Korelasi Antar Variabel Numerik', pad=20, size=16)
plt.xticks(rotation=45, ha='right', size=13)
plt.yticks(rotation=0, size=13)
plt.tight_layout()
plt.show()

sns.countplot(x='Attrition', data=df)
plt.title('Distribusi Target: Attrition')
plt.xlabel('Attrition (0 = Stay, 1 = Leave)')
plt.ylabel('Jumlah Karyawan')
plt.show()

sns.boxplot(x='Attrition', y='MonthlyIncome', data=df)
plt.title('Attrition vs MonthlyIncome')
plt.show()

sns.countplot(x='JobSatisfaction', hue='Attrition', data=df)
plt.title('JobSatisfaction vs Attrition')
plt.show()

sns.countplot(x='OverTime', hue='Attrition', data=df)
plt.title('OverTime vs Attrition')
plt.show()

sns.histplot(data=df, x='Age', kde=True, hue='Attrition', element='step')
plt.title('Distribusi Usia Berdasarkan Status Attrition')
plt.show()

sns.histplot(data=df, x='TotalWorkingYears', kde=True, hue='Attrition', element='step')
plt.title('Total Working Years vs Attrition')
plt.show()

plt.figure(figsize=(12,6))
sns.countplot(data=df, x='JobRole', hue='Attrition')
plt.title('Attrition Berdasarkan Job Role')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8,4))
sns.countplot(data=df, x='Department', hue='Attrition')
plt.title('Attrition Berdasarkan Department')
plt.show()

numerical_columns = [
    'Age', 'DailyRate', 'DistanceFromHome', 'MonthlyIncome', 'MonthlyRate',
    'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel',
    'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole',
    'YearsSinceLastPromotion', 'YearsWithCurrManager',
    # hasil feature engineering:
    'StabilityInRole', 'LoyaltyToManager', 'AvgTrainingPerYear',
    'AgeWhenStarted', 'AvgYearsPerCompany', 'IncomePerKm',
    'CompanyLoyalty', 'PromotionFrequency', 'AvgMonthlyIncomePerYear'
]

plt.figure(figsize=(16, 10))
df[numerical_columns].boxplot(rot=90)
plt.title('Boxplot Seluruh Kolom Numerik')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

selected_numerical = ['Age', 'MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany', 'Attrition']
sns.pairplot(df[selected_numerical], hue='Attrition')
plt.show()

# Pilih beberapa fitur numerik yang menarik berdasarkan korelasi
features = ['Age', 'MonthlyIncome', 'DistanceFromHome', 'TotalWorkingYears', 'AvgTrainingPerYear']

# Plot distribusi
plt.figure(figsize=(15, 10))
for i, col in enumerate(features, 1):
    plt.subplot(2, 3, i)
    sns.kdeplot(data=df[df['Attrition'] == 0], x=col, label='Stay', fill=True)
    sns.kdeplot(data=df[df['Attrition'] == 1], x=col, label='Left', fill=True)
    plt.title(f'Distribusi {col}')
    plt.legend()
plt.tight_layout()
plt.show()

correlations = df.corr(numeric_only=True)['Attrition'].drop('Attrition')
top_corr = correlations.sort_values(ascending=False)

print("Top 5 Korelasi Positif terhadap Attrition:\n", top_corr.head())
print("\nTop 5 Korelasi Negatif terhadap Attrition:\n", top_corr.tail())

"""#### Insight:

1. Terdapat korelasi negatif yang cukup kuat antara gaji dan insentif saham dengan tingkat pengunduran diri (attrition), menandakan bahwa faktor finansial memainkan peran krusial dalam mempertahankan karyawan.
2. Karyawan dengan jabatan lebih tinggi dan pengalaman kerja lebih lama cenderung lebih setia dan stabil, menegaskan pentingnya jalur pengembangan karier dalam mendorong retensi.
3. Walau dampaknya tidak sebesar faktor lain, frekuensi pelatihan tahunan dan jarak tempuh ke kantor turut memengaruhi keputusan untuk resign, yang bisa jadi mengindikasikan ketidakpuasan atau ketidaknyamanan dalam lingkungan kerja.
"""

# Menampilkan 10 baris pertama dari DataFrame
df.head(10)

"""## Modeling

### Logistic Regression
"""

X = df.drop(columns='Attrition')
y = df['Attrition']

X = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

imputer = SimpleImputer(strategy='median')  # median agar tahan outlier
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg_scaled = LogisticRegression(max_iter=5000, class_weight='balanced')
logreg_scaled.fit(X_train_scaled, y_train)

# Prediksi
y_pred_scaled = logreg_scaled.predict(X_test_scaled)
y_prob_scaled = logreg_scaled.predict_proba(X_test_scaled)[:, 1]

"""### Random Forest"""

rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf.fit(X_train_scaled, y_train)

y_pred_rf = rf.predict(X_test_scaled)
y_prob_rf = rf.predict_proba(X_test_scaled)[:, 1]

"""### XGBoost"""

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=1.0)
xgb.fit(X_train_scaled, y_train)

y_pred_xgb = xgb.predict(X_test_scaled)
y_prob_xgb = xgb.predict_proba(X_test_scaled)[:, 1]

"""### Gradient Boosting"""

gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train_scaled, y_train)

y_pred_gb = gb.predict(X_test_scaled)
y_prob_gb = gb.predict_proba(X_test_scaled)[:, 1]

"""### SVM"""

svc = SVC(probability=True, class_weight='balanced', random_state=42)
svc.fit(X_train_scaled, y_train)

y_pred_svc = svc.predict(X_test_scaled)
y_prob_svc = svc.predict_proba(X_test_scaled)[:, 1]

"""### LightGBM"""

lgbm = LGBMClassifier(random_state=42, class_weight='balanced')
lgbm.fit(X_train_scaled, y_train)

y_pred_lgbm = lgbm.predict(X_test_scaled)
y_prob_lgbm = lgbm.predict_proba(X_test_scaled)[:, 1]

"""### CatBoost"""

catboost = CatBoostClassifier(random_seed=42, verbose=0)
catboost.fit(X_train_scaled, y_train)

y_pred_cat = catboost.predict(X_test_scaled)
y_prob_cat = catboost.predict_proba(X_test_scaled)[:, 1]

"""## Evaluation

### Logistic Regression
"""

print("=== Classification Report ===")
print(classification_report(y_test, y_pred_scaled))

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_scaled))

"""### Random Forest"""

print("=== Random Forest ===")
print(classification_report(y_test, y_pred_rf))

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_rf))

"""### XGBoost"""

print("=== XGBoost ===")
print(classification_report(y_test, y_pred_xgb))
print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_xgb))

"""### Gradient Boosting"""

print("=== Gradient Boosting ===")
print(classification_report(y_test, y_pred_gb))
print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_gb))

"""### SVM"""

print("=== Support Vector Machine ===")
print(classification_report(y_test, y_pred_svc))
print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_svc))

"""### LightGBM"""

print("=== LightGBM ===")
print(classification_report(y_test, y_pred_lgbm))
print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_lgbm))

"""### CatBoost"""

print("\n=== CatBoost ===")
print(classification_report(y_test, y_pred_cat))
print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_cat))

"""
### Perbandingan Semua Model
"""

# Menghitung akurasi dari semua model yang telah dilatih
accuracy_scores = {
    'LogReg': accuracy_score(y_test, y_pred_scaled),
    'RandomForest': accuracy_score(y_test, y_pred_rf),
    'XGBoost': accuracy_score(y_test, y_pred_xgb),
    'GradBoost': accuracy_score(y_test, y_pred_gb),
    'SVM': accuracy_score(y_test, y_pred_svc),
    'LightGBM': accuracy_score(y_test, y_pred_lgbm),
    'CatBoost': accuracy_score(y_test, y_pred_cat)
}

# Mengurutkan akurasi model dari terendah ke tertinggi
accuracy_scores = dict(sorted(accuracy_scores.items(), key=lambda x: x[1]))

# Membuat visualisasi bar plot
plt.figure(figsize=(12, 6))
plt.bar(list(accuracy_scores.keys()), list(accuracy_scores.values()), color='#1f77b4')
plt.ylabel('Accuracy')
plt.title('Perbandingan Akurasi Antar Model (Urut dari Terendah ke Tertinggi)')
plt.xticks(rotation=45)
plt.ylim(0, 1)

# Menambahkan label nilai akurasi di atas setiap bar
for i, (model, score) in enumerate(accuracy_scores.items()):
    plt.text(i, score + 0.02, f'{score:.2f}', ha='center', fontweight='bold')

# Menambahkan grid dan mengatur layout
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Menampilkan plot
plt.show()

"""#### Insight:
- Model **CatBoost** memiliki akurasi tertinggi di antara semua model yang diuji.
- Model **Logistic Regression** memiliki akurasi terendah, jauh di bawah model lain.
- Model **Random Forest**, **SVM**, **LightGBM**, **XGBoost**, dan **Gradient Boosting** memiliki performa yang mirip dan cukup tinggi, namun masih di bawah CatBoost.
- Secara umum, model berbasis boosting (CatBoost, XGBoost, LightGBM, Gradient Boosting) unggul dibandingkan model klasik seperti Logistic Regression dan Random Forest.

## Save Model dan Data
"""

# Menyimpan model CatBoost terbaik
with open('best_model_catboost.pkl', 'wb') as f_model:
    pickle.dump(catboost, f_model)

# Menyimpan scaler untuk normalisasi data
with open('scaler.pkl', 'wb') as f_scaler:
    pickle.dump(scaler, f_scaler)

# Menyimpan imputer untuk menangani missing value
with open('imputer.pkl', 'wb') as f_imputer:
    pickle.dump(imputer, f_imputer)

# Menyimpan daftar kolom fitur yang digunakan dalam pelatihan
with open('feature_columns.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)

print("Model, Scaler, dan Imputer berhasil disimpan.")

df.to_csv('employee_data_sb.csv', index=False)
df.info()

employee_df = pd.read_csv('employee_data_sb.csv')
employee_df.info()

"""### Menyimpan Data ke Database

Melakukan penyimpanan data ke database PostgreSQL di Supabase menggunakan SQLAlchemy dan pandas.
"""

# Import library SQLAlchemy untuk koneksi database
from sqlalchemy import create_engine

# Konfigurasi url koneksi ke database PostgreSQL di Supabase
URL = "postgresql://postgres.bjkgormhqewziogrquin:indonesiajaya@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres"

# Inisialisasi koneksi database
engine = create_engine(URL)

# Menyimpan DataFrame ke tabel 'employee' di database
# Jika tabel belum ada, akan dibuat secara otomatis
# Jika sudah ada, data akan ditambahkan
employee_df.to_sql('employee', engine)

"""### Mengambil Data dari Database

Melakukan pengambilan data dari database PostgreSQL di Supabase menggunakan SQLAlchemy dan pandas.
"""

# Import library yang dibutuhkan untuk koneksi database
import pandas as pd
from sqlalchemy import create_engine

# Menyimpan URL koneksi ke database Supabase PostgreSQL
URL = "postgresql://postgres.bjkgormhqewziogrquin:indonesiajaya@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres"

# Membuat koneksi engine database menggunakan SQLAlchemy
engine = create_engine(URL)

# Mengambil data dari tabel 'employee' dan menyimpannya ke DataFrame
employee_sb_df = pd.read_sql_table(table_name="employee", con=engine.connect())

# Menampilkan 5 baris pertama dari data yang diambil
employee_sb_df.head(5)